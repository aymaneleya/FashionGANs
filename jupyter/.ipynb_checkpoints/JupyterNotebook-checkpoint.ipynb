{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we unzip the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Zipfile' from 'zipfile' (c:\\users\\nikolay dobrev\\appdata\\local\\programs\\python\\python37\\lib\\zipfile.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8e500a0f4ec8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mZipfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_debug\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mZipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Zipfile' from 'zipfile' (c:\\users\\nikolay dobrev\\appdata\\local\\programs\\python\\python37\\lib\\zipfile.py)"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name = data_debug.zip\n",
    "with ZipFile(file_name,'r') as zip:\n",
    "    zip.extractall()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then we define our utility methods\n",
    "### First is coming the picture helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmented_image_7_s_tilde(batch_size,s_tilde):\n",
    "    \n",
    "    img_no_downsample = s_tilde\n",
    "    #img_no_downsample_copy = copy.deepcopy(img_no_downsample)\n",
    "    #Create a tensor 4D, one dimention for each label [0,3]\n",
    "    img_3_layer_tensor = img_no_downsample[:,0:3,:,:]\n",
    "    # take the mean of the indecies > 2\n",
    "    img_4th_layer_tensor = torch.mean(img_no_downsample[:,4:,:,:], dim=1)\n",
    "    img_4th_layer_tensor = img_4th_layer_tensor.view(batch_size,1,128,128)\n",
    "    img_4_layer_tensor = torch.cat([img_3_layer_tensor, img_4th_layer_tensor], dim = 1)\n",
    "    return img_4_layer_tensor\n",
    "\n",
    "def get_segmented_image_7(seg_img):\n",
    "    img_no_downsample = seg_img\n",
    "    img_no_downsample_copy = copy.deepcopy(img_no_downsample)\n",
    "    #Create a tensor 4D, one dimention for each label [0,3]\n",
    "    img_7_lay_tensor = torch.zeros(7, 128, 128)\n",
    "\n",
    "\n",
    "    for i in range(0,7):\n",
    "        # first set the value when the index != label to a dummy value = 4\n",
    "        img_no_downsample_copy[img_no_downsample_copy != i] = 8\n",
    "        # set the value = 1 when the index = label \n",
    "        img_no_downsample_copy[img_no_downsample_copy == i] = 1\n",
    "        # convert from numpty array to tensor\n",
    "        img_no_downsample_copy = torch.from_numpy(img_no_downsample_copy)\n",
    "        #put the tensor in the corresponding index\n",
    "        img_7_lay_tensor[i, :, :] = img_no_downsample_copy\n",
    "        # reset the img_no_downsample_copy in order to process the second label \n",
    "        img_no_downsample_copy = copy.deepcopy(img_no_downsample)\n",
    "\n",
    "    #replace the dummy value in the tensor with 0\n",
    "    img_7_lay_tensor[img_7_lay_tensor == 8] = 0\n",
    "\n",
    "    # convert the tensot to double instead of unit8 (usigned integer)\n",
    "    img_7_lay_tensor = img_7_lay_tensor.type(torch.DoubleTensor)\n",
    "    img_7_lay_tensor = img_7_lay_tensor.resize(1,7,128,128)\n",
    "\n",
    "    img_7_lay_tensor = img_7_lay_tensor.resize(7,128,128)\n",
    "    img_7_lay_tensor = img_7_lay_tensor.permute(0,2,1)\n",
    "        \n",
    "    return img_7_lay_tensor\n",
    "\n",
    "def get_downsampled_image_4(img):\n",
    "    img_no_downsample = img\n",
    "    img_no_downsample_copy = copy.deepcopy(img_no_downsample)\n",
    "    # LÂ´ : only use the first four lables. if label > 3, set label = 3\n",
    "    img_no_downsample[img_no_downsample > 3] = 3\n",
    "    #Create a tensor 4D, one dimention for each label [0,3]\n",
    "    img_4_lay_tensor = torch.zeros(4, 128, 128)\n",
    "\n",
    "    for i in range(0,4):\n",
    "        # first set the value when the index != label to a dummy value = 4\n",
    "        img_no_downsample_copy[img_no_downsample_copy != i] = 4\n",
    "        # set the value = 1 when the index = label \n",
    "        img_no_downsample_copy[img_no_downsample_copy == i] = 1\n",
    "        # convert from numpty array to tensor\n",
    "        img_no_downsample_copy = torch.from_numpy(img_no_downsample_copy)\n",
    "        #put the tensor in the corresponding index\n",
    "        img_4_lay_tensor[i, :, :] = img_no_downsample_copy\n",
    "        # reset the img_no_downsample_copy in order to process the second label \n",
    "        img_no_downsample_copy = copy.deepcopy(img_no_downsample)\n",
    "\n",
    "    #replace the dummy value in the tensor with 0\n",
    "    img_4_lay_tensor[img_4_lay_tensor == 4] = 0\n",
    "\n",
    "    # convert the tensot to double instead of unit8 (usigned integer)\n",
    "    img_4_lay_tensor = img_4_lay_tensor.type(torch.DoubleTensor)\n",
    "    img_4_lay_tensor = img_4_lay_tensor.resize(1,4,128,128)\n",
    "    #segmented_image = segmented_image.view(1,-1,-1,-1)\n",
    "    # downsampling by 1/8\n",
    "    img_4_lay_tensor = torch.nn.functional.interpolate(img_4_lay_tensor, scale_factor=(0.0625, 0.0625),  mode='bicubic', align_corners=True)\n",
    "\n",
    "    img_4_lay_tensor = img_4_lay_tensor.resize(4,8,8)\n",
    "    img_4_lay_tensor = img_4_lay_tensor.permute(0,2,1)\n",
    "        \n",
    "    return img_4_lay_tensor\n",
    "\n",
    "def get_downsampled_image_4_mS0(batch_size,img):\n",
    "    img_no_downsample = img\n",
    "\n",
    "    img_4_lay_tensor = torch.nn.functional.interpolate(img_no_downsample, scale_factor=(0.0625, 0.0625),  mode='bicubic', align_corners=True)\n",
    "\n",
    "    img_4_lay_tensor = img_4_lay_tensor.resize(batch_size,4,8,8)\n",
    "        \n",
    "    return img_4_lay_tensor\n",
    "\n",
    "\n",
    "def plot_tensor_image(t_img):\n",
    "    for i in range(len(t_img)):\n",
    "        segmented_tensor = t_img[i,:,:].resize(8,8)\n",
    "        plt.imshow(segmented_tensor)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_tensor_seg_image(t_img):\n",
    "    for i in range(len(t_img)):\n",
    "        segmented_tensor = t_img[i,:,:].resize(128,128)\n",
    "        plt.imshow(segmented_tensor)\n",
    "        plt.show()\n",
    "\n",
    "def get_downsampled_batch(batchsize,batch):\n",
    "    batch_np = batch.cpu().data.numpy()\n",
    "    batch_down_sampled = torch.ones(batchsize, 4, 8,8)\n",
    "    for i in range(batchsize):\n",
    "        batch_down_sampled[i]=get_downsampled_image_4(batch_np[i])\n",
    "    return batch_down_sampled\n",
    "\n",
    "def get_segmented_batch(batchsize,batch):\n",
    "    batch_np = batch.cpu().data.numpy()\n",
    "    batch_segmented = torch.ones(batchsize, 7, 128,128)\n",
    "    for i in range(batchsize):\n",
    "        batch_segmented[i]=get_segmented_image_7(batch_np[i])\n",
    "    return batch_segmented\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we upload the data_loader- data loader is not used in this jupyter notebook but it holds useful signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4ed7df6991d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlanguage_original_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'..'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'language_original.mat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mindeces_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'..'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ind.mat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msegmented_images_raw_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'..'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'segmented_images.p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import scipy.io\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "## Fix here if needed\n",
    "# language_original_path = os.path.join(os.path.dirname(__file__),'..','data','language_original.mat')\n",
    "# indeces_path = os.path.join(os.path.dirname(__file__),'..','data','ind.mat')\n",
    "# segmented_images_raw_path = os.path.join(os.path.dirname(__file__),'..','data','segmented_images.p')\n",
    "# real_images_raw_path = os.path.join(os.path.dirname(__file__),'..','data','real_images.p')\n",
    "# h5_file_path = os.path.join(os.path.dirname(__file__),'..','data','G2.h5')\n",
    "# lang_encoding = os.path.join(os.path.dirname(__file__),'..','data','encode.npy')\n",
    "\n",
    "def binary_representaiton(val, n_bits):\n",
    "    binary_str = bin(val)[2:]\n",
    "    while n_bits > len(binary_str):\n",
    "            binary_str = '0'+binary_str\n",
    "    arr =[]\n",
    "    for i in range(len(binary_str)):\n",
    "        arr.append(float(binary_str[i]))\n",
    "    return arr\n",
    "\n",
    "\n",
    "def create_mask_for_skin_tone(segmented_image):\n",
    "    mask = copy.deepcopy(segmented_image)\n",
    "    for i in range(7):\n",
    "        if(2 == i or 6 == i):\n",
    "            mask[i == mask] = 1\n",
    "        else:\n",
    "            mask[i==mask] = 0\n",
    "    return mask\n",
    "\n",
    "def apply_mask(segmented_image,real_image):\n",
    "    mask = create_mask_for_skin_tone(segmented_image)\n",
    "    masked = copy.deepcopy(real_image)\n",
    "    for channel in range(len(real_image)):\n",
    "        masked[channel] = np.multiply(masked[channel],mask)\n",
    "    return masked\n",
    "\n",
    "\n",
    "class FashionData(Dataset):\n",
    "    def __init__(self,X,y,type_of_data):\n",
    "        self.X = X[type_of_data]\n",
    "        self.y = y[type_of_data]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        design_encoding = []\n",
    "        design_encoding.append(float(self.X['gender'][index]))\n",
    "        design_encoding.extend(binary_representaiton(self.X['color'][index],5))\n",
    "        design_encoding.extend(binary_representaiton(self.X['sleeve'][index],3))\n",
    "        design_encoding.extend(binary_representaiton(self.X['cate_new'][index],5))\n",
    "        design_encoding.append(self.X['r'][index])\n",
    "        design_encoding.append(self.X['g'][index])\n",
    "        design_encoding.append(self.X['b'][index])\n",
    "        design_encoding.append(self.X['y'][index])\n",
    "        design_encoding.extend(self.X['encoding'][index])\n",
    "        design_encoding = np.array(design_encoding)\n",
    "\n",
    "        return (design_encoding,self.X['down_sampled_images'][index],self.X['segmented_image'][index],self.y[index])        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    \n",
    "\n",
    "# should we normalize the real_images\n",
    "def construct_data(segmented_images,real_images,indeces,language,encoded_values):\n",
    "    X = {}\n",
    "    y = {}\n",
    "\n",
    "    X['train'] = {}\n",
    "    X['train']['gender'] =[]\n",
    "    X['train']['color'] =[]\n",
    "    X['train']['sleeve'] =[]\n",
    "    X['train']['cate_new'] =[]\n",
    "    X['train']['segmented_image'] = []\n",
    "    X['train']['down_sampled_images'] = []\n",
    "    X['train']['description'] = []\n",
    "    X['train']['encoding'] = []\n",
    "    X['train']['codeJ'] = []\n",
    "    X['train']['r'] = []\n",
    "    X['train']['g'] = []\n",
    "    X['train']['b'] = []\n",
    "    X['train']['y'] = []\n",
    "\n",
    "    X['test'] = {}\n",
    "    X['test']['gender'] =[]\n",
    "    X['test']['color'] =[]\n",
    "    X['test']['sleeve'] =[]\n",
    "    X['test']['cate_new'] =[]\n",
    "    X['test']['segmented_image'] = []\n",
    "    X['test']['down_sampled_images'] = []\n",
    "    X['test']['description'] = []\n",
    "    X['test']['encoding'] = []\n",
    "    X['test']['codeJ'] = []\n",
    "    X['test']['r'] = []\n",
    "    X['test']['g'] = []\n",
    "    X['test']['b'] = []\n",
    "    X['test']['y'] = []\n",
    "\n",
    "    y['train'] = []\n",
    "    y['test'] = []\n",
    "\n",
    "    # length_to_iterate_train = len(indeces['train_ind'])\n",
    "    length_to_iterate_train = 10000\n",
    "    # length_to_iterate_test = len(indeces['test_ind'])\n",
    "    length_to_iterate_test = 1000\n",
    "\n",
    "\n",
    "    for i in range(length_to_iterate_train):\n",
    "        idx = indeces['train_ind'][i][0] - 1\n",
    "        X['train']['gender'].append(language['gender_'][idx][0])\n",
    "        X['train']['color'].append(language['color_'][idx][0])\n",
    "        X['train']['sleeve'].append(language['sleeve_'][idx][0])\n",
    "        X['train']['cate_new'].append(language['cate_new'][idx][0])\n",
    "        X['train']['description'].append(str(language['engJ'][idx][0][0]))\n",
    "        X['train']['encoding'].append(encoded_values[idx])\n",
    "        # X['train']['segmented_image'].append(segmented_images[idx])\n",
    "        X['train']['segmented_image'].append(get_segmented_image_7(segmented_images[idx]))  \n",
    "        X['train']['codeJ'].append(str(language['codeJ'][idx][0][0]))\n",
    "        skin_tone = apply_mask(np.reshape(segmented_images[idx],(128,128)),real_images[idx])\n",
    "\n",
    "        r,g,b = np.median(skin_tone[0]), np.median(skin_tone[1]), np.median(skin_tone[2])\n",
    "\n",
    "        X['train']['r'].append(r)\n",
    "        X['train']['g'].append(g)\n",
    "        X['train']['b'].append(b)\n",
    "        X['train']['y'].append(0.2125*r + 0.7154*g +  0.0721*b)\n",
    "        #X['train']['down_sampled_images'].append(get_downsampled_image(segmented_images[idx][0]))\n",
    "        X['train']['down_sampled_images'].append(get_downsampled_image_4(segmented_images[idx]))\n",
    "        y['train'].append(real_images[idx])\n",
    "\n",
    "    for i in range(length_to_iterate_test):\n",
    "        idx = indeces['test_ind'][i][0] - 1\n",
    "        X['test']['gender'].append(language['gender_'][idx][0])\n",
    "        X['test']['color'].append(language['color_'][idx][0])\n",
    "        X['test']['sleeve'].append(language['sleeve_'][idx][0])\n",
    "        X['test']['cate_new'].append(language['cate_new'][idx][0])\n",
    "        X['test']['description'].append(str(language['engJ'][idx][0][0]))\n",
    "        X['test']['encoding'].append(encoded_values[idx])\n",
    "        X['test']['segmented_image'].append(get_segmented_image_7(segmented_images[idx]))  \n",
    "        # X['test']['segmented_image'].append(segmented_images[idx])\n",
    "        X['test']['codeJ'].append(str(language['codeJ'][idx][0][0]))\n",
    "        skin_tone = apply_mask(np.reshape(segmented_images[idx],(128,128)),real_images[idx])\n",
    "\n",
    "        r,g,b = np.median(skin_tone[0]), np.median(skin_tone[1]), np.median(skin_tone[2])\n",
    "\n",
    "        X['test']['r'].append(r)\n",
    "        X['test']['g'].append(g)\n",
    "        X['test']['b'].append(b)\n",
    "        X['test']['y'].append(0.2125*r + 0.7154*g +  0.0721*b)\n",
    "\n",
    "        X['test']['down_sampled_images'].append(get_downsampled_image_4(segmented_images[idx]))\n",
    "\n",
    "        y['test'].append(real_images[idx])\n",
    "    \n",
    "    return (X,y)\n",
    "\n",
    "    \n",
    "def normalize_pictures(real_images):\n",
    "    for image in real_images:\n",
    "        for channel in range(len(image)):\n",
    "            image[channel] = (image[channel] - image[channel].min()) / (image[channel].max()-image[channel].min())\n",
    "    return real_images\n",
    "\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    segmented_images = None\n",
    "    real_images = None\n",
    "    print(\"Check if the serialized data is present\")\n",
    "    # check if the serialized images are present if not create them\n",
    "    if not(os.path.isfile(segmented_images_raw_path) and os.path.isfile(real_images_raw_path)):\n",
    "        print(\"We have to read the h5 file, it will take time\")\n",
    "        with h5py.File(h5_file_path, 'r') as f:   \n",
    "\n",
    "            # Get the data\n",
    "            # Segmentated images 1x 128x 128 values from 0 to 6\n",
    "            segmented_images = list(f['b_'])\n",
    "            # Real images three channels instad of 0-255 values for a pixel we have normalized values between [-1;1]\n",
    "            pickle.dump(segmented_images, open(segmented_images_raw_path, 'wb')) \n",
    "            real_images = list(f['ih'])\n",
    "            #normalize the real images\n",
    "            real_images = normalize_pictures(real_images)\n",
    "            pickle.dump(real_images, open(real_images_raw_path, 'wb')) \n",
    "            print(\"H5 read and data has been serialized\")\n",
    "    if None == segmented_images:\n",
    "        infile = open(segmented_images_raw_path,'rb')\n",
    "        segmented_images = pickle.load(infile)\n",
    "        infile.close()\n",
    "\n",
    "    if None == real_images:\n",
    "        real_images = pickle.load(open(real_images_raw_path,'rb'))\n",
    "    print(\"Images has been loaded successfully\")\n",
    "    print(\"Now reading .mat files\")\n",
    "    # now read language\n",
    "    lang_org = scipy.io.loadmat(language_original_path)\n",
    "\n",
    "    # read the indeces as well\n",
    "    indeces = scipy.io.loadmat(indeces_path)\n",
    "\n",
    "    print(\"Everything is loaded now constructing the dictionaries\")\n",
    "    \n",
    "    encoded_values = np.load(lang_encoding)\n",
    "\n",
    "    (X,y) = construct_data(segmented_images,real_images,indeces,lang_org, encoded_values)\n",
    "    print(\"Data constructed\")\n",
    "    print(\"Pickle the data\")\n",
    "    handle = open(os.path.join(os.path.dirname(__file__),'..','data','debug_data.pkl'),'wb')\n",
    "    pickle.dump((X,y), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    handle.close()\n",
    "    \n",
    "    return (X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the definitions of Discriminators and Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#sizes\n",
    "human_attributes_size = 18\n",
    "encoded_description_size = 100\n",
    "flatten_down_sampled_segmentations_size = 256\n",
    "gausian_noise_size = 100\n",
    "\n",
    "design_encoding = human_attributes_size + encoded_description_size + gausian_noise_size # 218\n",
    "\n",
    "\n",
    "\n",
    "class Generator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self).__init__()\n",
    "\n",
    "        self.G1_Layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=design_encoding,out_channels=1024,kernel_size=4,stride=4,padding=0),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.G1_A = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4,out_channels=64,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.G1_LastLayers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=(128+512),out_channels=256,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=64,out_channels=7,kernel_size=4,stride=2,padding=1),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_design_desc, x_down_sampled_image):\n",
    "        x_design = self.G1_Layer2(x_design_desc)\n",
    "        x_down_sampled = self.G1_A(x_down_sampled_image)\n",
    "        concatenated = torch.cat((x_design,x_down_sampled),1)\n",
    "        return self.G1_LastLayers(concatenated)\n",
    "\n",
    "class Generator2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.G2_Layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=design_encoding,out_channels=1024,kernel_size=4,stride=4,padding=0),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.G2_LayerC = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=7,out_channels=64,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.G2_Layer6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=128, kernel_size=4, stride=2, padding=1),            \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_design_desc, s_tilde):\n",
    "        l3 = self.G2_Layer3(x_design_desc)\n",
    "        lc = self.G2_LayerC(s_tilde)\n",
    "        concatenated = torch.cat((l3,lc),dim=1)\n",
    "        return self.G2_Layer6(concatenated)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self).__init__()\n",
    "\n",
    "        self.D1_Layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=7,out_channels=64,kernel_size=4,stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.D1_condition = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4,out_channels=64,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.D1_concatenation = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=640,out_channels=1024,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.D1_LastLayers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1142,out_channels=1024,kernel_size=1,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=1024,out_channels=1,kernel_size=4,stride=4,padding=0),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_segmented_image,x_down_sampled, x_design_encoding):\n",
    "        layer4 = self.D1_Layer1(x_segmented_image)\n",
    "        x_down_sampled = self.D1_condition(x_down_sampled)\n",
    "        \n",
    "        concatenated = torch.cat((layer4,x_down_sampled),1)\n",
    "        layer5 = self.D1_concatenation(concatenated)\n",
    "        d_by_4 = x_design_encoding.repeat(1,16)\n",
    "        d_by_4 = d_by_4.view(layer5.shape[0],x_design_encoding.shape[1],4,4)\n",
    "        input_for_layer6 = torch.cat((layer5,d_by_4),1)\n",
    "        output = self.D1_LastLayers(input_for_layer6)\n",
    "        output = output.view(x_segmented_image.shape[0],1)\n",
    "        return self.sigmoid(output)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self).__init__()\n",
    "        self.D2_Layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),            \n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2)            \n",
    "        )\n",
    "\n",
    "        self.D2_LayerC = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=7,out_channels=64,kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),            \n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),            \n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.D2_Layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2)         \n",
    "        )\n",
    "\n",
    "        self.D2_Layer7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1142,out_channels=1024,kernel_size=1, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.Conv2d(in_channels=1024,out_channels=1,kernel_size=4, stride=4, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, design_encoding, g2_output, s_tilde):\n",
    "        l3 = self.D2_Layer3(g2_output)\n",
    "        lc = self.D2_LayerC(s_tilde)\n",
    "        concatenated = torch.cat((l3.lc),dim=1)\n",
    "        l5 = self.D2_Layer5(concatenated)\n",
    "        l7 = self.D2_Layer7(l5)\n",
    "        return self.sigmoid(l7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loader\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "X, y = None,None\n",
    "\n",
    "# release\n",
    "# if os.path.isfile(os.path.join(os.path.dirname(__file__),'..','data','data.pkl')):\n",
    "#     with open(os.path.join(os.path.dirname(__file__),'..','data','data.pkl')) as handle:\n",
    "#         X,y = pickle.load(handle)\n",
    "\n",
    "# debug\n",
    "loaded_data = None\n",
    "# if os.path.isfile(os.path.join(os.path.dirname(__file__),'..','data','debug_data_10k.pkl')):\n",
    "#     with open(os.path.join(os.path.dirname(__file__),'..','data','debug_data_10k.pkl'),'rb') as handle:\n",
    "if os.path.isfile(os.path.join(os.path.dirname(__file__),'..','data','debug_data.pkl')):\n",
    "    with open(os.path.join(os.path.dirname(__file__),'..','data','debug_data.pkl'),'rb') as handle:\n",
    "        loaded_data = pickle.load(handle)\n",
    "        X,y = loaded_data[0],loaded_data[1]\n",
    "else:\n",
    "    X, y = load_data()\n",
    "training_data = FashionData(X,y,'train')\n",
    "testing_data = FashionData(X,y,'test')\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size,pin_memory=cuda)\n",
    "test_loader  = DataLoader(testing_data, batch_size=batch_size, pin_memory=cuda)\n",
    "\n",
    "flatten_image_size = 128*128\n",
    "\n",
    "latent_dim_g2 = flatten_image_size + GANs.gausian_noise_size + GANs.human_attributes_size # 16492\n",
    "\n",
    "G1 = GANs.Generator1()\n",
    "D1 = GANs.Discriminator1()\n",
    "if cuda:\n",
    "    G1.cuda()\n",
    "    D1.cuda()\n",
    "\n",
    "loss = torch.nn.BCELoss()\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "generator_1_optim = torch.optim.Adam(G1.parameters(), 2e-4, betas=(0.5, 0.999))\n",
    "discriminator_1_optim = torch.optim.Adam(D1.parameters(), 2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "\n",
    "# S0: segmented image\n",
    "# mS0: downsampled segmented image\n",
    "# d: designed encoding\n",
    "# dz: {d,z} encoding and noise\n",
    "# y: real image\n",
    "#TODO assign data to these variables above\n",
    "#TODO whether create variables for each step or just once?\n",
    "#TODO should batch works?\n",
    "#TODO debug the language encoder function\n",
    "#TODO encode all data or run encoder during training\n",
    "#TODO whether need to add condition loss in updating G2?\n",
    "tmp_img = \"tmp_gan_out.png\"\n",
    "discriminator_loss, generator_loss = [], []\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    batch_d_loss, batch_g_loss = [], []\n",
    "    \n",
    "    for i , data in enumerate(train_loader, 0):\n",
    "        \n",
    "        d, mS0, S0, label = data\n",
    "        \n",
    "        true_label = torch.ones(batch_size, 1).to(device)\n",
    "        fake_label = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "        D1.zero_grad()\n",
    "        G1.zero_grad()\n",
    "\n",
    "        \n",
    "        \n",
    "        #################### Update D #############################\n",
    "        # loss 1. real image + real condition -> 1\n",
    "        x_true_S0 = Variable(S0).to(device,dtype=torch.float)\n",
    "        x_true_mS0 = Variable(mS0).to(device,dtype=torch.float)\n",
    "        x_true_d = Variable(d).to(device,dtype=torch.float)        \n",
    "        output = D1.forward(x_true_S0,x_true_mS0,x_true_d)\n",
    "        \n",
    "        error_true = loss(output, true_label) \n",
    "        error_true.backward()\n",
    "\n",
    "        # loss 2. sampled real image + wrong condition -> 0\n",
    "        # shuffle d     \n",
    "        # shuffle the true d row wise\n",
    "        x_notmatch_d = x_true_d[torch.randperm(x_true_d.size()[0])]\n",
    "        \n",
    "        x_notmatch_d = Variable(x_notmatch_d).to(device)    \n",
    "        output = D1.forward(x_true_S0 ,x_true_mS0,x_notmatch_d)\n",
    "\n",
    "        error_notmatch = loss(output, fake_label) \n",
    "        error_notmatch.backward()\n",
    "\n",
    "        # loss 3. generated fake image + real condition -> 0s\n",
    "        # z = torch.randn(batch_size, 100, 1, 1,dtype=torch.float64)\n",
    "        z = torch.randn(batch_size, 100,dtype=torch.float64)\n",
    "        dz = torch.cat([d, z] , dim=1)\n",
    "        dz = dz.view((batch_size,dz.shape[1],1,1))\n",
    "        dz = Variable(dz).to(device,dtype=torch.float)\n",
    "        x_g_mS0 = Variable(mS0).to(device,dtype=torch.float)\n",
    "\n",
    "        S_tilde = G1.forward(dz,x_g_mS0)\n",
    "\n",
    "        x_fake_S = S_tilde\n",
    "        # x_fake_S = Variable(S_tilde).to(device)\n",
    "        mS_tilde = down_sample.get_segmented_image_7_s_tilde(batch_size, S_tilde)\n",
    "        x_fake_mS = down_sample.get_downsampled_image_4_mS0(batch_size, mS_tilde)\n",
    "        x_fake_mS = Variable(x_fake_mS).to(device,dtype=torch.float)\n",
    "        x_fake_d = Variable(d).to(device,dtype=torch.float) \n",
    "        output = D1.forward(x_fake_S.detach(),x_fake_mS.detach(),x_fake_d.detach())\n",
    "        \n",
    "\n",
    "        error_fake = loss(output, fake_label)#log(1-log(g(z)))\n",
    "        error_fake.backward()\n",
    "        discriminator_1_optim.step()\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        #################### Update G #############################\n",
    "        \n",
    "        \n",
    "        # Step 4. Send fake data through discriminator _again_\n",
    "        #         propagate the error of the generator and\n",
    "        #         update G weights.\n",
    "        output = D1.forward(x_fake_S,x_fake_mS,x_fake_d)\n",
    "        \n",
    "        error_generator = loss(output, true_label)\n",
    "        error_generator.backward()\n",
    "        generator_1_optim.step()\n",
    "        \n",
    "        # batch_d_loss.append((error_true/(error_true + error_fake + error_notmatch)).item())\n",
    "        batch_d_loss.append((error_true + error_fake + error_notmatch).item())\n",
    "        batch_g_loss.append(error_generator.item())\n",
    "\n",
    "    discriminator_loss.append(np.mean(batch_d_loss))\n",
    "    generator_loss.append(np.mean(batch_g_loss))\n",
    "    \n",
    "##################################\n",
    "    print('Training epoch %d: discriminator_loss = %.5f, generator_loss = %.5f' % (epoch, discriminator_loss[epoch].item(), generator_loss[epoch].item()))\n",
    "\n",
    "s   \n",
    "    # Generate data\n",
    "\n",
    "    with torch.no_grad():\n",
    "        zsample = torch.randn(100,dtype=torch.float64)\n",
    "        dsample = []\n",
    "        dsample.append(float(X['train']['gender'][0]))\n",
    "        dsample.extend(data_loader.binary_representaiton(X['train']['color'][0],5))\n",
    "        dsample.extend(data_loader.binary_representaiton(X['train']['sleeve'][0],3))\n",
    "        dsample.extend(data_loader.binary_representaiton(X['train']['cate_new'][0],5))\n",
    "        dsample.append(X['train']['r'][0])\n",
    "        dsample.append(X['train']['g'][0])\n",
    "        dsample.append(X['train']['b'][0])\n",
    "        dsample.append(X['train']['y'][0])\n",
    "        dsample.extend(X['train']['encoding'][0])\n",
    "        dsample = np.array(dsample)\n",
    "        dsample = torch.from_numpy(dsample)\n",
    "        dzsample = torch.cat([dsample,zsample] , dim=0)\n",
    "        dzsample = dzsample.view((1,dzsample.shape[0],1,1))\n",
    "        dzsample = Variable(dzsample).to(device,dtype=torch.float)\n",
    "        mS0_sample = X['train']['down_sampled_images'][0]\n",
    "        mS0_sample = mS0_sample.view((1,4,8,8))\n",
    "        mS0_sample = Variable(mS0_sample).to(device,dtype=torch.float)\n",
    "        S_tilde_sample = G1.forward(dzsample,mS0_sample)\n",
    "    \n",
    "\n",
    "\n",
    "plt.plot(range(num_epochs), discriminator_loss)\n",
    "plt.show()\n",
    "plt.plot(range(num_epochs), generator_loss)\n",
    "plt.show()\n",
    "\n",
    "S_tilde_sample = S_tilde_sample.data.cpu().numpy()\n",
    "S_tilde_sample =  S_tilde_sample.reshape(7,128,128)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(S_tilde_sample[1])\n",
    "\n",
    "S_0_sample = X['train']['segmented_image'][0].data.cpu().numpy()\n",
    "S_0_sample =S_0_sample.reshape(7,128,128)\n",
    "ax= fig.add_subplot(122)\n",
    "ax.imshow(S_0_sample[1])\n",
    "plt.show()\n",
    "\n",
    "#######save_image#########\n",
    "\n",
    "\n",
    "# plt.savefig(tmp_img)\n",
    "# plt.close(f)\n",
    "# display(Image(filename=tmp_img))\n",
    "# clear_output(wait=True)\n",
    "\n",
    "# os.remove(tmp_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
